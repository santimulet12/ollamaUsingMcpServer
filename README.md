# Sistema de Chat con Herramientas MCP

Sistema de asistente conversacional que integra Ollama con herramientas MCP (Model Context Protocol) para obtener informaciÃ³n en tiempo real, como el clima de ciudades.

## ğŸ“‹ Requisitos Previos

- Python 3.8 o superior
- Ollama instalado y ejecutÃ¡ndose
- Modelo Qwen 2.5:3b descargado en Ollama
- ConexiÃ³n a internet

## ğŸ”§ InstalaciÃ³n

### 1. Clonar o descargar el proyecto

AsegÃºrate de tener los archivos `MCPServer.py` y `API.py` en el mismo directorio.

### 2. Instalar dependencias

```bash
pip install fastmcp requests flask uvicorn
```

### 3. Configurar Ollama

AsegÃºrate de que Ollama estÃ© instalado y ejecutÃ¡ndose. Si no lo tienes, instÃ¡lalo desde [ollama.ai](https://ollama.ai).

Descarga el modelo requerido:

```bash
ollama pull qwen2.5:3b
```

Verifica que Ollama estÃ© corriendo:

```bash
ollama list
```

### 4. Configurar la API

En el archivo `API.py`, modifica la lÃ­nea 9 con la direcciÃ³n IP correcta de tu servidor Ollama:

```python
OLLAMA_API_URL = "http://IP_AQUI:11434/api/chat"
```

Si Ollama estÃ¡ en la misma mÃ¡quina, usa:

```python
OLLAMA_API_URL = "http://localhost:11434/api/chat"
```

## ğŸš€ EjecuciÃ³n

### Paso 1: Iniciar el Servidor MCP

En una terminal, ejecuta:

```bash
python MCPServer.py
```

DeberÃ­as ver:

```
ğŸš€ Servidor MCP iniciado en http://localhost:8000/mcp
ğŸ“¡ Herramientas disponibles: obtener_clima
```

### Paso 2: Iniciar la API Flask

En otra terminal (dejando el servidor MCP corriendo), ejecuta:

```bash
python API.py
```

La API Flask se iniciarÃ¡ en `http://localhost:5000`

## ğŸ“¡ Uso de la API

### Endpoint de Chat

**URL:** `POST http://localhost:5000/ask-ia`

**Body (JSON):**

```json
{
  "prompt": "Â¿CÃ³mo estÃ¡ el clima en Buenos Aires?",
  "historial": []
}
```

**Ejemplo con historial:**

```json
{
  "prompt": "Â¿Y en Madrid?",
  "historial": [
    {"role": "user", "content": "Â¿CÃ³mo estÃ¡ el clima en Buenos Aires?"},
    {"role": "assistant", "content": "En Buenos Aires hace 22Â°C..."}
  ]
}
```

**Respuesta:**

```json
{
  "response": "En Buenos Aires actualmente hay 22Â°C con cielo despejado. La sensaciÃ³n tÃ©rmica es de 21Â°C y hay un 65% de humedad."
}
```

### Endpoint para velificar el estado de la API

**URL:** `GET http://localhost:5000/health`

**Respuesta:**

```json
{
  "status": "ok"
}
```

## ğŸ› ï¸ Herramientas Disponibles

### obtener_clima

Obtiene informaciÃ³n meteorolÃ³gica actual de cualquier ciudad del mundo.

**ParÃ¡metros:**
- `ciudad` (string): Nombre de la ciudad (ej: "Buenos Aires", "Madrid", "New York")

**InformaciÃ³n retornada:**
- Temperatura actual y sensaciÃ³n tÃ©rmica
- DescripciÃ³n del clima
- Humedad
- Velocidad del viento
- PresiÃ³n atmosfÃ©rica
- Visibilidad
- Ãndice UV

## ğŸ§ª Ejemplos de Uso

### Usando cURL:

```bash
curl -X POST http://localhost:5000/ask-ia \
  -H "Content-Type: application/json" \
  -d '{"prompt": "Â¿QuÃ© tiempo hace en Mendoza?"}'
```

### Usando Python:

```python
import requests

response = requests.post(
    "http://localhost:5000/ask-ia",
    json={
        "prompt": "Â¿CÃ³mo estÃ¡ el clima en Londres?",
        "historial": []
    }
)

print(response.json()["response"])
```

## ğŸ› SoluciÃ³n de Problemas

### El servidor MCP no inicia

- Verifica que el puerto 8000 no estÃ© en uso
- AsegÃºrate de tener instalado `uvicorn`: `pip install uvicorn`

### La API Flask no se conecta a Ollama

- Verifica que Ollama estÃ© corriendo: `ollama list`
- Confirma que la URL en `API.py` sea correcta
- AsegÃºrate de que el modelo `qwen2.5:3b` estÃ© descargado

### Error de conexiÃ³n al servidor MCP

- Verifica que `MCPServer.py` estÃ© corriendo
- Confirma que estÃ© accesible en `http://localhost:8000/mcp`

### El modelo no usa las herramientas correctamente

- El modelo debe responder en el formato exacto: `USE_TOOL:` y `ARGS:`
- Si no funciona, considera ajustar el `SYS_PROMPT` en `API.py`

## ğŸ“ Notas

- El servidor MCP debe estar corriendo antes de iniciar la API Flask
- El servicio de clima usa `wttr.in`, que es gratuito pero puede tener lÃ­mites de uso
- Las respuestas estÃ¡n configuradas para ser concisas (3-4 oraciones mÃ¡ximo)
- Todo el sistema estÃ¡ configurado para responder en espaÃ±ol
